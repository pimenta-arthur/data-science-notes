{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Richert, W., & Coelho, L.P. (2013). **Building Machine Learning Systems with Python**. Birmingham: Livery Place.\n",
    "\n",
    "You can find most of the examples in the book and other are from my understanding of the subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. A Brief Introduction of a Nearest Neighbor Classifier\n",
    "A new sample is classified by calculating the distance to the nearest training case; the sign of that point then determines the classification of the sample.<br>\n",
    "If we consider that each sample is represented by its features (in mathematical terms, as a point in N-dimensional space), we can compute the distance between samples. <br>\n",
    "Euclidean distance = $\\sqrt{\\sum_{i=1}^{n} (p_i-q_i)^2}$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset (Iris dataset)\n",
    "Overall, this includes 150 samples 50 in each of three classes. <br>Attribute Information, features: **sepal length, sepal width, petal length, petal width**, and classes: **Iris-Setosa, Iris-Versicolour, Iris-Virginica**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the data with load_iris from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = iris_data['data']\n",
    "target = iris_data['target']\n",
    "target_names = iris_data['target_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts the data into a data frame for a better understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target.reshape(150,1)\n",
    "data = np.hstack([features, target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=data, columns=['Sepal.Length','Sepal.Width','Petal.Length','Petal.Width','Species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal.Length</th>\n",
       "      <th>Sepal.Width</th>\n",
       "      <th>Petal.Length</th>\n",
       "      <th>Petal.Width</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.054000</td>\n",
       "      <td>3.758667</td>\n",
       "      <td>1.198667</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.433594</td>\n",
       "      <td>1.764420</td>\n",
       "      <td>0.763161</td>\n",
       "      <td>0.819232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sepal.Length  Sepal.Width  Petal.Length  Petal.Width     Species\n",
       "count    150.000000   150.000000    150.000000   150.000000  150.000000\n",
       "mean       5.843333     3.054000      3.758667     1.198667    1.000000\n",
       "std        0.828066     0.433594      1.764420     0.763161    0.819232\n",
       "min        4.300000     2.000000      1.000000     0.100000    0.000000\n",
       "25%        5.100000     2.800000      1.600000     0.300000    0.000000\n",
       "50%        5.800000     3.000000      4.350000     1.300000    1.000000\n",
       "75%        6.400000     3.300000      5.100000     1.800000    2.000000\n",
       "max        7.900000     4.400000      6.900000     2.500000    2.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "Calculates the Euclidean distance between two points in a N-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(p, q):\n",
    "    'Computes squared euclidean distance'\n",
    "    return sqrt(np.sum((p-q)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when classifying, we adopt a simple rule: given a new sample, we look at\n",
    "the dataset for the point that is closest to it (its nearest neighbor) and look at its label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_classefy(training_set, training_labels, new_sample):\n",
    "    for t in training_set:\n",
    "        dists = np.array([distance(t, new_sample) for t in training_set])\n",
    "    nearest = dists.argmin()\n",
    "    return target_names[training_labels[nearest]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given sample better fit in the setosa class\n"
     ]
    }
   ],
   "source": [
    "'Predicts the class of a given sample'\n",
    "new_sample = np.array([3,0,2,0.4])\n",
    "label = nn_classefy(features, target, new_sample).take(0)\n",
    "print('The given sample better fit in the %s class' %(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, note that this model performs perfectly on its training data! For each point, its closest neighbor is itself, and so its label matches perfectly (unless two examples have exactly the same features but different labels, which can happen). Therefore, it is essential to test using a cross-validation protocol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PS:** As you may have noticed we did not take into account the units of the features, sometimes it can be an problem because we may be summing up different kinds of units and mixing up them like lengths, areas, and dimensionless quantities (which is something you never want to do in a physical system). We need to normalize all of the features to a common scale. There are many solutions to this problem; a simple one is to normalize to Z-scores. The Z-score of a value is how far away from the mean it is in terms of units of standard deviation. It comes down to this simple pair of operations: <br>\n",
    "- subtract the mean for each feature:\n",
    "        features -= features.mean(axis=0)\n",
    "- divide each feature by its standard deviation:\n",
    "       features /= features.std(axis=0)\n",
    "\n",
    "Independent of what the original values were, after Z-scoring, a value of zero is the mean and positive values are above the mean and negative values are below it. The nearest neighbor classifier is simple, but sometimes good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 k-NN Classifier\n",
    "The k-NN classifier extends the idea previously discussed in **2.1 A Brief Introduction of a Nearest Neighbor Classifier** by considering not just the closest point but the k closest points. All k neighbors vote to select the label. k is typically a small number and odd to break ties, such as 3 or 5, but can be larger, particularly if the dataset is very large. Larger k values help reduce the effects of noisy points within the training data set, and the choice of k is often performed through cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting with k-nearest neighbor (k-NN) algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Finding Related Posts with a Naive Approach (Bag-of-words)\n",
    "The bag-of-word approach uses simple word counts as its basis. For each word in the post, its occurrence is counted and noted in a vector. Not surprisingly, this step is also called vectorization. The vector is typically huge as it contains as many elements as the words that occur in the whole dataset. <br>\n",
    "So let us pick a random post, for which we will then create the count vector. We will then compare its distance to all the count vectors and fetch the post with the smallest one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset\n",
    "Let us play with the dataset consisting of the following posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = {\n",
    "    0: \"I can only imagine how difficult this is for you.\",\n",
    "    1: \"Can you imagine that?\",\n",
    "    2: \"I can't imagine what he was thinking to hide a thing like that from you.\",\n",
    "    3: \"Imagine that you personally had to create everything you wanted to use.\",\n",
    "    4: \"He could imagine her horror when she discovered what he planned.\",\n",
    "    5: \"Then imagine if you shared your Digital Echo with a billion other people on the planet.\",\n",
    "    6: \"He cannot imagine how very, very happy he will be when he can tell us his thoughts, and we can tell him how we have loved him so long.\", \n",
    "    7: \"I imagine it would taste mighty good.\",\n",
    "    8: \"I can just imagine what a funny figure that policeman cut!\",\n",
    "    9: \"The winter's better here than Europe, I imagine, he said with a smile.\",\n",
    "    10: \"Can you imagine a world without poverty?\",\n",
    "    11: \"I couldn't imagine you'd take that long for a dog walk.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post dataset, we want to find the most similar post for the short given post \"Can you imagine all the people smiling?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting text to vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extending the vectorizer with NLTK's stemmer. We need to stem the posts before we feed them into **TfidfVectorizer**. Notice we could use just **CountVectorizer**, however we'd not count the term frequencies for every post, and in addition, discounting those that appear in many posts. <br>\n",
    "In other words, we want a high value for a given term in a given value if that term occurs often in that particular post and very rarely anywhere else. <br>\n",
    "The resulting document vectors will not contain counts any more. Instead, they will contain the individual TF-IDF values per term. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Equivalent to CountVectorizer followed by TfidfTransformer'\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk.stem\n",
    "\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our current text preprocessing phase includes the following steps:\n",
    "1. Lower casing the raw post in the preprocessing step (done in the parent class).\n",
    "2. Extracting all individual words in the tokenization step (done in the parent class).\n",
    "3. Converting each word into its stemmed version.\n",
    "4. Throwing away words that occur way too often to be of any help in detecting relevant posts.\n",
    "5. Throwing away words that occur so seldom that there is only a small chance that they occur in future posts.\n",
    "6. Counting the remaining words.\n",
    "7. Calculating TF-IDF values from the counts, considering the whole text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples: 12, features: 42\n"
     ]
    }
   ],
   "source": [
    "vectorizer = StemmedTfidfVectorizer(min_df=1, \n",
    "                                    stop_words='english', \n",
    "                                    decode_error='ignore')\n",
    "X_train = vectorizer.fit_transform(posts.values())\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"samples: %d, features: %d\" % (num_samples, num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we have 12 posts with a total of 42 different words. The following words that have been tokenized will be counted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['better', 'billion', 'couldn', 'creat', 'cut', 'difficult', 'digit', 'discov', 'dog', 'echo', 'europ', 'figur', 'funni', 'good', 'happi', 'hide', 'horror', 'imagin', 'just', 'like', 'long', 'love', 'mighti', 'peopl', 'person', 'plan', 'planet', 'policeman', 'poverti', 'said', 'share', 'smile', 'tast', 'tell', 'thing', 'think', 'thought', 'use', 'walk', 'want', 'winter', 'world']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picks a random new post to find related posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinate matrix\n",
      "  (0, 41)\t0.5660249087784507\n",
      "  (0, 31)\t0.5660249087784507\n",
      "  (0, 23)\t0.5660249087784507\n",
      "  (0, 17)\t0.1970974579415979\n",
      "\n",
      "Full array\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.19709746\n",
      "  0.         0.         0.         0.         0.         0.56602491\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.56602491 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.56602491]]\n"
     ]
    }
   ],
   "source": [
    "new_post = \"Imagine all the people in the world smiling?\"\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "print(\"Coordinate matrix\")\n",
    "print(new_post_vec)\n",
    "print()\n",
    "print(\"Full array\")\n",
    "print(new_post_vec.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that count vectors returned by the **transform** method are sparse. That is, each vector does not store one count value for each word, as most of those counts would be zero (post does not contain the word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, using only the counts of the raw words is too simple. We will have to normalize them to get vectors of unit length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'calculates the Euclidean distance between the count vectors of the new post and all the old posts'\n",
    "import scipy as sp\n",
    "\n",
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1/sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2/sp.linalg.norm(v2.toarray())\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **norm( )** function calculates the Euclidean norm (shortest distance). With **dist_norm**, we just need to iterate over all the posts and remember the nearest one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post 0 with dist 1.37: I can only imagine how difficult this is for you.\n",
      "Post 1 with dist 1.27: Can you imagine that?\n",
      "Post 2 with dist 1.39: I can't imagine what he was thinking to hide a thing like that from you.\n",
      "Post 3 with dist 1.39: Imagine that you personally had to create everything you wanted to use.\n",
      "Post 4 with dist 1.39: He could imagine her horror when she discovered what he planned.\n",
      "Post 5 with dist 1.22: Then imagine if you shared your Digital Echo with a billion other people on the planet.\n",
      "Post 6 with dist 1.40: He cannot imagine how very, very happy he will be when he can tell us his thoughts, and we can tell him how we have loved him so long.\n",
      "Post 7 with dist 1.39: I imagine it would taste mighty good.\n",
      "Post 8 with dist 1.39: I can just imagine what a funny figure that policeman cut!\n",
      "Post 9 with dist 1.20: The winter's better here than Europe, I imagine, he said with a smile.\n",
      "Post 10 with dist 1.06: Can you imagine a world without poverty?\n",
      "Post 11 with dist 1.39: I couldn't imagine you'd take that long for a dog walk.\n",
      "\n",
      "Best post is 10 with dist 1.06\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "best_doc = None\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "\n",
    "for i in range(num_samples):\n",
    "    post = posts[i]\n",
    "    post_vec = X_train.getrow(i)\n",
    "    d = dist_norm(new_post_vec, post_vec)\n",
    "    print(\"Post %i with dist %.2f: %s\" % (i, d, post))\n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(\"\\nBest post is %i with dist %.2f\"%(best_i, best_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this process, we are able to convert a bunch of noisy text into a concise representation of feature values.\n",
    "But, as simple and as powerful as the bag-of-words approach with its extensions is, it has some drawbacks that we should be aware of. They are as follows:\n",
    "- It does not cover word relations. With the previous vectorization approach, the text \"Car hits wall\" and \"Wall hits car\" will both have the same feature vector.\n",
    "- It does not cover word relations. With the previous vectorization approach, the text \"Car hits wall\" and \"Wall hits car\" will both have the same feature vector.\n",
    "- It totally fails with misspelled words. Although it is clear to the readers that \"database\" and \"databas\" convey the same meaning, our approach will treat them as totally different words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. KMeans\n",
    "KMeans is the most widely used flat clustering algorithm. After it is initialized with the desired number of clusters, num_clusters, it maintains that number of so-called cluster centroids. Initially, it would pick any of the num_clusters posts and set the centroids to their feature vector. Then it would go through all other posts and assign them the nearest centroid as their current cluster. Then it will move each centroid into the middle of all the vectors of that particular class. This changes, of course, the cluster assignment. Some posts are now nearer to another cluster. So it will update the assignments for those changed posts. This is done as long as the centroids move a considerable amount. After some iterations, the movements will fall below a threshold and we consider clustering to be converged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset (20newsgroup)\n",
    "One standard dataset in machine learning is the 20newsgroup dataset, which contains 18,826 posts from 20 different newsgroups. Among the groups' topics are technical ones such as comp.sys.mac.hardware or sci.crypt as well as more politics- and religion-related ones such as talk.politics.guns or soc.religion. christian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "groups = ['comp.graphics', 'comp.os.ms-windows.misc', \n",
    "          'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', \n",
    "          'comp.windows.x', 'sci.space']\n",
    "\n",
    "train_data = fetch_20newsgroups(subset='train', categories=groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples: 3529, features: 4712\n"
     ]
    }
   ],
   "source": [
    "vectorizer = StemmedTfidfVectorizer(min_df=10, \n",
    "                                    max_df=0.5, \n",
    "                                    stop_words='english',\n",
    "                                    decode_error='ignore')\n",
    "vectorized = vectorizer.fit_transform(train_data.data)\n",
    "num_samples, num_features = vectorized.shape\n",
    "print(\"samples: %d, features: %d\" % (num_samples, num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a pool of **3529** posts and extracted for each of them a feature vector of **47121** dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 5946.006\n",
      "Iteration  1, inertia 3217.040\n",
      "Iteration  2, inertia 3171.778\n",
      "Iteration  3, inertia 3146.622\n",
      "Iteration  4, inertia 3130.727\n",
      "Iteration  5, inertia 3120.660\n",
      "Iteration  6, inertia 3114.887\n",
      "Iteration  7, inertia 3112.594\n",
      "Iteration  8, inertia 3110.610\n",
      "Iteration  9, inertia 3109.233\n",
      "Iteration 10, inertia 3108.062\n",
      "Iteration 11, inertia 3107.435\n",
      "Iteration 12, inertia 3106.950\n",
      "Iteration 13, inertia 3106.753\n",
      "Iteration 14, inertia 3106.490\n",
      "Iteration 15, inertia 3106.276\n",
      "Iteration 16, inertia 3106.191\n",
      "Iteration 17, inertia 3106.098\n",
      "Iteration 18, inertia 3105.802\n",
      "Iteration 19, inertia 3105.290\n",
      "Iteration 20, inertia 3104.788\n",
      "Iteration 21, inertia 3104.657\n",
      "Iteration 22, inertia 3104.621\n",
      "Converged at iteration 22: center shift 0.000000e+00 within tolerance 2.069005e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='random', max_iter=300,\n",
       "    n_clusters=50, n_init=1, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "'fix the cluster size to 50'\n",
    "num_clusters = 50\n",
    "km = KMeans(n_clusters=num_clusters, init='random', n_init=1,\n",
    "   verbose=1)\n",
    "km.fit(vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every vectorized post that has been fit, there is a corresponding integer label in **km.labels_**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26  9 44 ... 13 48  9]\n",
      "(3529,)\n"
     ]
    }
   ],
   "source": [
    "print(km.labels_)\n",
    "print(km.labels_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving our initial challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assign a cluster to a newly arriving post using **km.predict**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disk drive problems. Hi, I have a problem with my hard disk. After 1 year it is working only sporadically now. I tried to format it, but now it doesn't boot any more. Any ideas? Thanks.\n"
     ]
    }
   ],
   "source": [
    "new_post = \"Disk drive problems. Hi, I have a problem with my hard disk. After 1 year it is working only sporadically now. I tried to format it, but now it doesn't boot any more. Any ideas? Thanks.\"\n",
    "print(new_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectorize this post before we predict its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post_vec = vectorizer.transform([new_post])\n",
    "new_post_label = km.predict(new_post_vec)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the clustering, we do not need to compare **new_post_vec** to all post vectors. Instead, we can focus only on the posts of the same cluster. <br>\n",
    "The comparison in the bracket results in a Boolean array, and **nonzero** converts that\n",
    "array into a smaller array containing the indices of the **True** elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  69  125  139  157  167  201  213  228  247  297  308  359  377  395\n",
      "  428  463  520  531  533  580  581  650  661  676  779  806  807  808\n",
      "  882  905  935  939  944  964  971  976 1076 1114 1228 1246 1266 1311\n",
      " 1427 1431 1486 1495 1548 1608 1716 1752 1806 1809 1852 1853 1864 1893\n",
      " 1986 1990 1996 2013 2015 2061 2085 2133 2151 2235 2257 2270 2339 2347\n",
      " 2351 2414 2447 2463 2482 2493 2512 2516 2518 2525 2565 2573 2600 2619\n",
      " 2624 2639 2667 2678 2705 2745 2791 2800 2842 2875 2907 2956 2964 2993\n",
      " 3018 3080 3111 3145 3199 3202 3225 3241 3278 3285 3296 3297 3309 3310\n",
      " 3437 3458]\n",
      "114\n"
     ]
    }
   ],
   "source": [
    "similar_indices = (km.labels_==new_post_label).nonzero()[0]\n",
    "print(similar_indices)\n",
    "print(len(similar_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **similar_indices**, we then simply have to build a list of posts together with\n",
    "their similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n"
     ]
    }
   ],
   "source": [
    "similar = []\n",
    "\n",
    "for i in similar_indices:\n",
    "    dist = sp.linalg.norm((new_post_vec - vectorized[i]).toarray())\n",
    "    similar.append((dist, train_data.data[i])) \n",
    "    \n",
    "similar = sorted(similar)\n",
    "print(len(similar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found **137** posts in the cluster of our post. To give the user a quick idea of what kind of similar posts are available, we can now present the most similar post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines shows the posts together with their similarity values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position 1\n",
      "Similarity 1.0378441731334074\n",
      "\n",
      "From: Thomas Dachsel <GERTHD@mvs.sas.com>\n",
      "Subject: BOOT PROBLEM with IDE controller\n",
      "Nntp-Posting-Host: sdcmvs.mvs.sas.com\n",
      "Organization: SAS Institute Inc.\n",
      "Lines: 25\n",
      "\n",
      "Hi,\n",
      "I've got a Multi I/O card (IDE controller + serial/parallel\n",
      "interface) and two floppy drives (5 1/4, 3 1/2) and a\n",
      "Quantum ProDrive 80AT connected to it.\n",
      "I was able to format the hard disk, but I could not boot from\n",
      "it. I can boot from drive A: (which disk drive does not matter)\n",
      "but if I remove the disk from drive A and press the reset switch,\n",
      "the LED of drive A: continues to glow, and the hard disk is\n",
      "not accessed at all.\n",
      "I guess this must be a problem of either the Multi I/o card\n",
      "or floppy disk drive settings (jumper configuration?)\n",
      "Does someone have any hint what could be the reason for it.\n",
      "Please reply by email to GERTHD@MVS.SAS.COM\n",
      "Thanks,\n",
      "Thomas\n",
      "+-------------------------------------------------------------------+\n",
      "| Thomas Dachsel                                                    |\n",
      "| Internet: GERTHD@MVS.SAS.COM                                      |\n",
      "| Fidonet:  Thomas_Dachsel@camel.fido.de (2:247/40)                 |\n",
      "| Subnet:   dachsel@rnivh.rni.sub.org (UUCP in Germany, now active) |\n",
      "| Phone:    +49 6221 4150 (work), +49 6203 12274 (home)             |\n",
      "| Fax:      +49 6221 415101                                         |\n",
      "| Snail:    SAS Institute GmbH, P.O.Box 105307, D-W-6900 Heidelberg |\n",
      "| Tagline:  One bad sector can ruin a whole day...                  |\n",
      "+-------------------------------------------------------------------+\n",
      "\n",
      "############################################################################################################\n",
      "Position 2\n",
      "Similarity 1.2834616717125098\n",
      "\n",
      "From: ebosco@us.oracle.com (Eric Bosco)\n",
      "Subject: Help adding a SCSI Drive\n",
      "Nntp-Posting-Host: monica.us.oracle.com\n",
      "Reply-To: ebosco@us.oracle.com\n",
      "Organization: Oracle Corp., Redwood Shores CA\n",
      "X-Disclaimer: This message was written by an unauthenticated user\n",
      "              at Oracle Corporation.  The opinions expressed are those\n",
      "              of the user and not necessarily those of Oracle.\n",
      "Lines: 35\n",
      "\n",
      "\n",
      "I have a 486sx25 computer with a 105 Mg Seagate IDE drive and a controler  \n",
      "built into the motherboard. I want to add a SCSI drive (a quantum prodrive  \n",
      "425F 425 MG formatted). I have no documentation at all and I need your  \n",
      "help!\n",
      "\n",
      "As I understand it, here is the process of adding such a drive.  Could you  \n",
      "please tell me if I'm right..\n",
      "\n",
      "1- Buy a SCSI contoler.  Which one? I know Adaptec is good, but they are  \n",
      "kind of expensive.  Are there any good boards in the $100 region? I want  \n",
      "it to be compatible with OS2 and Unix if possible.  Also, I have seen on  \n",
      "the net that there are SCSI and SCSI2 drives. Is this true? Does the  \n",
      "adapter need to be the same as the drive? What type of drive is the  \n",
      "quantum?\n",
      "\n",
      "2- connect the drive to the adapter via a SCSI cable and the power cable.\n",
      "Do i have to worry about the power supply? I think I have 200 watts and  \n",
      "all I'm powering are two floppies and the seagate drive.\n",
      "\n",
      "3- Setup the BIOS to recognize the drive as the second drive.  What type  \n",
      "of drive is this? I don't have the numbers for this drive.\n",
      "\n",
      "4- Format and create partitions on the drive. Do I use format or fdisk? I  \n",
      "think that IDE drives can't be low-level formatted. Is it the same with  \n",
      "SCSI? How exactly does fdisk work? I have a reduced msdos 5.0 manual  \n",
      "(clone obliges) and there is no mention of fdisk.  Ideally, I would want  \n",
      "the drive partitioned in to two partitions D: and E: how do I do this?\n",
      "\n",
      "Well that seems to be all. Is there anythiing I'm forgetting? \n",
      "Any help is *really* appreciated, I'm lost...\n",
      "\n",
      "-Eric\n",
      "\n",
      "ebosco@us.oracle.com\n",
      "\n",
      "############################################################################################################\n",
      "Position 3\n",
      "Similarity 1.3889842415462028\n",
      "\n",
      "From: thorf@csa.bu.edu (Thor Farrish)\n",
      "Subject: Maxtor drive geometry/jumpers\n",
      "Distribution: usa\n",
      "Organization: Computer Science Department, Boston University, Boston, MA, USA\n",
      "Lines: 1\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_at_1 = similar[0]\n",
    "show_at_2 = similar[len(similar)//2]\n",
    "show_at_3 = similar[-1]\n",
    "\n",
    "print('Position', 1)\n",
    "print('Similarity', show_at_1[0])\n",
    "print()\n",
    "print(show_at_1[1])\n",
    "print('#'*108)\n",
    "print('Position', 2)\n",
    "print('Similarity', show_at_2[0])\n",
    "print()\n",
    "print(show_at_2[1])\n",
    "print('#'*108)\n",
    "print('Position', 3)\n",
    "print('Similarity', show_at_3[0])\n",
    "print()\n",
    "print(show_at_3[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 LDL - Latent Dirichlet Allocation (Topic Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "\n",
    "corpus = corpora.BleiCorpus('ap/ap.dat', 'ap/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics= 15\n",
    "model = models.ldamodel.LdaModel(corpus,\n",
    "                                 num_topics=num_topics,\n",
    "                                 id2word=corpus.id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.84087586), (11, 0.028474718), (13, 0.12752445)]\n"
     ]
    }
   ],
   "source": [
    "topics = [model[c] for c in corpus]\n",
    "print(topics[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.03336257),\n",
       " (1, 0.033362597),\n",
       " (2, 0.03336253),\n",
       " (3, 0.033362478),\n",
       " (4, 0.03336254),\n",
       " (5, 0.03336253),\n",
       " (6, 0.033362556),\n",
       " (7, 0.033362493),\n",
       " (8, 0.03336264),\n",
       " (9, 0.033362567),\n",
       " (10, 0.03336269),\n",
       " (11, 0.0333626),\n",
       " (12, 0.033362556),\n",
       " (13, 0.033362538),\n",
       " (14, 0.5329241)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_document_topics(topics[400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.0054774974),\n",
       " (1, 0.004466451),\n",
       " (2, 0.017002746),\n",
       " (4, 0.007841692),\n",
       " (5, 0.0037883585),\n",
       " (6, 0.0037225352),\n",
       " (7, 0.0035408465),\n",
       " (11, 0.0038979158),\n",
       " (21, 0.0075890017),\n",
       " (255, 0.0041134753)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(model.get_topic_terms(num_topics-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[model.get_document_topics[topic] for topic in topics if len(model.ge == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.013474172), (2, 0.010262474)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_term_topics(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {\"car\": [\"a\", \"b\", \"a\", \"c\", \"b\"], \"class\": [1,2,1,3,2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  car  class\n",
       "0   a      1\n",
       "1   b      2\n",
       "2   a      1\n",
       "3   c      3\n",
       "4   b      2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'car': ['a', 'b', 'a', 'c', 'b'], 'class': [1, 2, 1, 3, 2]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regiment</th>\n",
       "      <th>company</th>\n",
       "      <th>name</th>\n",
       "      <th>preTestScore</th>\n",
       "      <th>postTestScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nighthawks</td>\n",
       "      <td>1st</td>\n",
       "      <td>Miller</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nighthawks</td>\n",
       "      <td>1st</td>\n",
       "      <td>Jacobson</td>\n",
       "      <td>24</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nighthawks</td>\n",
       "      <td>2nd</td>\n",
       "      <td>Ali</td>\n",
       "      <td>31</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nighthawks</td>\n",
       "      <td>2nd</td>\n",
       "      <td>Milner</td>\n",
       "      <td>2</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dragoons</td>\n",
       "      <td>1st</td>\n",
       "      <td>Cooze</td>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dragoons</td>\n",
       "      <td>1st</td>\n",
       "      <td>Jacon</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dragoons</td>\n",
       "      <td>2nd</td>\n",
       "      <td>Ryaner</td>\n",
       "      <td>24</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dragoons</td>\n",
       "      <td>2nd</td>\n",
       "      <td>Sone</td>\n",
       "      <td>31</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Scouts</td>\n",
       "      <td>1st</td>\n",
       "      <td>Sloan</td>\n",
       "      <td>2</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Scouts</td>\n",
       "      <td>1st</td>\n",
       "      <td>Piger</td>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Scouts</td>\n",
       "      <td>2nd</td>\n",
       "      <td>Riani</td>\n",
       "      <td>2</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Scouts</td>\n",
       "      <td>2nd</td>\n",
       "      <td>Ali</td>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      regiment company      name  preTestScore  postTestScore\n",
       "0   Nighthawks     1st    Miller             4             25\n",
       "1   Nighthawks     1st  Jacobson            24             94\n",
       "2   Nighthawks     2nd       Ali            31             57\n",
       "3   Nighthawks     2nd    Milner             2             62\n",
       "4     Dragoons     1st     Cooze             3             70\n",
       "5     Dragoons     1st     Jacon             4             25\n",
       "6     Dragoons     2nd    Ryaner            24             94\n",
       "7     Dragoons     2nd      Sone            31             57\n",
       "8       Scouts     1st     Sloan             2             62\n",
       "9       Scouts     1st     Piger             3             70\n",
       "10      Scouts     2nd     Riani             2             62\n",
       "11      Scouts     2nd       Ali             3             70"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = {'regiment': ['Nighthawks', 'Nighthawks', 'Nighthawks', 'Nighthawks', 'Dragoons', 'Dragoons', 'Dragoons', 'Dragoons', 'Scouts', 'Scouts', 'Scouts', 'Scouts'], \n",
    "        'company': ['1st', '1st', '2nd', '2nd', '1st', '1st', '2nd', '2nd','1st', '1st', '2nd', '2nd'], \n",
    "        'name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze', 'Jacon', 'Ryaner', 'Sone', 'Sloan', 'Piger', 'Riani', 'Ali'], \n",
    "        'preTestScore': [4, 24, 31, 2, 3, 4, 24, 31, 2, 3, 2, 3],\n",
    "        'postTestScore': [25, 94, 57, 62, 70, 25, 94, 57, 62, 70, 62, 70]}\n",
    "df = pd.DataFrame(raw_data, columns = ['regiment', 'company', 'name', 'preTestScore', 'postTestScore'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dragoons': {'company': ['1st', '1st', '2nd', '2nd'], 'name': ['Cooze', 'Jacon', 'Ryaner', 'Sone']}, 'Nighthawks': {'company': ['1st', '1st', '2nd', '2nd'], 'name': ['Miller', 'Jacobson', 'Ali', 'Milner']}, 'Scouts': {'company': ['1st', '1st', '2nd', '2nd'], 'name': ['Sloan', 'Piger', 'Riani', 'Ali']}}\n"
     ]
    }
   ],
   "source": [
    "x = df.groupby('regiment')[['company','name']].apply(lambda x: x.to_dict(orient='list')).to_dict()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dragoons</th>\n",
       "      <th>Nighthawks</th>\n",
       "      <th>Scouts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>company</th>\n",
       "      <td>[1st, 1st, 2nd, 2nd]</td>\n",
       "      <td>[1st, 1st, 2nd, 2nd]</td>\n",
       "      <td>[1st, 1st, 2nd, 2nd]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>[Cooze, Jacon, Ryaner, Sone]</td>\n",
       "      <td>[Miller, Jacobson, Ali, Milner]</td>\n",
       "      <td>[Sloan, Piger, Riani, Ali]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Dragoons                       Nighthawks  \\\n",
       "company          [1st, 1st, 2nd, 2nd]             [1st, 1st, 2nd, 2nd]   \n",
       "name     [Cooze, Jacon, Ryaner, Sone]  [Miller, Jacobson, Ali, Milner]   \n",
       "\n",
       "                             Scouts  \n",
       "company        [1st, 1st, 2nd, 2nd]  \n",
       "name     [Sloan, Piger, Riani, Ali]  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.DataFrame(x)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dragoons</th>\n",
       "      <td>[1st, 1st, 2nd, 2nd]</td>\n",
       "      <td>[Cooze, Jacon, Ryaner, Sone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nighthawks</th>\n",
       "      <td>[1st, 1st, 2nd, 2nd]</td>\n",
       "      <td>[Miller, Jacobson, Ali, Milner]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scouts</th>\n",
       "      <td>[1st, 1st, 2nd, 2nd]</td>\n",
       "      <td>[Sloan, Piger, Riani, Ali]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         company                             name\n",
       "Dragoons    [1st, 1st, 2nd, 2nd]     [Cooze, Jacon, Ryaner, Sone]\n",
       "Nighthawks  [1st, 1st, 2nd, 2nd]  [Miller, Jacobson, Ali, Milner]\n",
       "Scouts      [1st, 1st, 2nd, 2nd]       [Sloan, Piger, Riani, Ali]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
