{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello, here I take notes while reading the book: <br>\n",
    "Richert, W., & Coelho, L.P. (2013). **Building Machine Learning Systems with Python**. Birmingham: Livery Place.\n",
    "\n",
    "Obs: Some of the examples are from the book and others are from my own interpretation of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. A Brief Introduction of a Nearest Neighbor Classifier\n",
    "A new sample is classified by calculating the distance to the nearest training case; the sign of that point then determines the classification of the sample.<br>\n",
    "If we consider that each sample is represented by its features (in mathematical terms, as a point in N-dimensional space), we can compute the distance between samples. <br>\n",
    "Euclidean distance = $\\sqrt{\\sum_{i=1}^{n} (p_i-q_i)^2}$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset (Iris dataset)\n",
    "Overall, this includes 150 samples 50 in each of three classes. <br>Attribute Information, features: **sepal length, sepal width, petal length, petal width**, and classes: **Iris-Setosa, Iris-Versicolour, Iris-Virginica**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the data with load_iris from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = iris_data['data']\n",
    "target = iris_data['target']\n",
    "target_names = iris_data['target_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts the data into a data frame for a better understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target.reshape(150,1)\n",
    "data = np.hstack([features, target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=data, columns=['Sepal.Length','Sepal.Width','Petal.Length','Petal.Width','Species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal.Length</th>\n",
       "      <th>Sepal.Width</th>\n",
       "      <th>Petal.Length</th>\n",
       "      <th>Petal.Width</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.054000</td>\n",
       "      <td>3.758667</td>\n",
       "      <td>1.198667</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.433594</td>\n",
       "      <td>1.764420</td>\n",
       "      <td>0.763161</td>\n",
       "      <td>0.819232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sepal.Length  Sepal.Width  Petal.Length  Petal.Width     Species\n",
       "count    150.000000   150.000000    150.000000   150.000000  150.000000\n",
       "mean       5.843333     3.054000      3.758667     1.198667    1.000000\n",
       "std        0.828066     0.433594      1.764420     0.763161    0.819232\n",
       "min        4.300000     2.000000      1.000000     0.100000    0.000000\n",
       "25%        5.100000     2.800000      1.600000     0.300000    0.000000\n",
       "50%        5.800000     3.000000      4.350000     1.300000    1.000000\n",
       "75%        6.400000     3.300000      5.100000     1.800000    2.000000\n",
       "max        7.900000     4.400000      6.900000     2.500000    2.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "Calculates the Euclidean distance between two points in a N-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(p, q):\n",
    "    'Computes squared euclidean distance'\n",
    "    return sqrt(np.sum((p-q)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when classifying, we adopt a simple rule: given a new sample, we look at\n",
    "the dataset for the point that is closest to it (its nearest neighbor) and look at its label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_classefy(training_set, training_labels, new_sample):\n",
    "    for t in training_set:\n",
    "        dists = np.array([distance(t, new_sample) for t in training_set])\n",
    "    nearest = dists.argmin()\n",
    "    return target_names[training_labels[nearest]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given sample better fit in the setosa class\n"
     ]
    }
   ],
   "source": [
    "'Predicts the class of a given sample'\n",
    "new_sample = np.array([3,0,2,0.4])\n",
    "label = nn_classefy(features, target, new_sample).take(0)\n",
    "print('The given sample better fit in the %s class' %(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, note that this model performs perfectly on its training data! For each point, its closest neighbor is itself, and so its label matches perfectly (unless two examples have exactly the same features but different labels, which can happen). Therefore, it is essential to test using a cross-validation protocol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PS:** As you may have noticed we did not take into account the units of the features, sometimes it can be an problem because we may be summing up different kinds of units and mixing up them like lengths, areas, and dimensionless quantities (which is something you never want to do in a physical system). We need to normalize all of the features to a common scale. There are many solutions to this problem; a simple one is to normalize to Z-scores. The Z-score of a value is how far away from the mean it is in terms of units of standard deviation. It comes down to this simple pair of operations: <br>\n",
    "- subtract the mean for each feature:\n",
    "        features -= features.mean(axis=0)\n",
    "- divide each feature by its standard deviation:\n",
    "       features /= features.std(axis=0)\n",
    "\n",
    "Independent of what the original values were, after Z-scoring, a value of zero is the mean and positive values are above the mean and negative values are below it. The nearest neighbor classifier is simple, but sometimes good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 k-NN Classifier\n",
    "The k-NN classifier extends the idea previously discussed in **2.1 A Brief Introduction of a Nearest Neighbor Classifier** by considering not just the closest point but the k closest points. All k neighbors vote to select the label. k is typically a small number and odd to break ties, such as 3 or 5, but can be larger, particularly if the dataset is very large. Larger k values help reduce the effects of noisy points within the training data set, and the choice of k is often performed through cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting with k-nearest neighbor (k-NN) algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Finding Related Posts with a Naive Approach (Bag-of-words)\n",
    "The bag-of-word approach uses simple word counts as its basis. For each word in the post, its occurrence is counted and noted in a vector. Not surprisingly, this step is also called vectorization. The vector is typically huge as it contains as many elements as the words that occur in the whole dataset. <br>\n",
    "So let us pick a random post, for which we will then create the count vector. We will then compare its distance to all the count vectors and fetch the post with the smallest one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset\n",
    "Let us play with the dataset consisting of the following posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = {\n",
    "    0: \"I can only imagine how difficult this is for you.\",\n",
    "    1: \"Can you imagine that?\",\n",
    "    2: \"I can't imagine what he was thinking to hide a thing like that from you.\",\n",
    "    3: \"Imagine that you personally had to create everything you wanted to use.\",\n",
    "    4: \"He could imagine her horror when she discovered what he planned.\",\n",
    "    5: \"Then imagine if you shared your Digital Echo with a billion other people on the planet.\",\n",
    "    6: \"He cannot imagine how very, very happy he will be when he can tell us his thoughts, and we can tell him how we have loved him so long.\", \n",
    "    7: \"I imagine it would taste mighty good.\",\n",
    "    8: \"I can just imagine what a funny figure that policeman cut!\",\n",
    "    9: \"The winter's better here than Europe, I imagine, he said with a smile.\",\n",
    "    10: \"Can you imagine a world without poverty?\",\n",
    "    11: \"I couldn't imagine you'd take that long for a dog walk.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post dataset, we want to find the most similar post for the short given post \"Can you imagine all the people smiling?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting text to vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extending the vectorizer with NLTK's stemmer. We need to stem the posts before we feed them into **TfidfVectorizer**. Notice we could use just **CountVectorizer**, however we'd not count the term frequencies for every post, and in addition, discounting those that appear in many posts. <br>\n",
    "In other words, we want a high value for a given term in a given value if that term occurs often in that particular post and very rarely anywhere else. <br>\n",
    "The resulting document vectors will not contain counts any more. Instead, they will contain the individual TF-IDF values per term. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Equivalent to CountVectorizer followed by TfidfTransformer'\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk.stem\n",
    "\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our current text preprocessing phase includes the following steps:\n",
    "1. Lower casing the raw post in the preprocessing step (done in the parent class).\n",
    "2. Extracting all individual words in the tokenization step (done in the parent class).\n",
    "3. Converting each word into its stemmed version.\n",
    "4. Throwing away words that occur way too often to be of any help in detecting relevant posts.\n",
    "5. Throwing away words that occur so seldom that there is only a small chance that they occur in future posts.\n",
    "6. Counting the remaining words.\n",
    "7. Calculating TF-IDF values from the counts, considering the whole text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples: 12, features: 42\n"
     ]
    }
   ],
   "source": [
    "vectorizer = StemmedTfidfVectorizer(min_df=1, \n",
    "                                    stop_words='english', \n",
    "                                    decode_error='ignore')\n",
    "X_train = vectorizer.fit_transform(posts.values())\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"samples: %d, features: %d\" % (num_samples, num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we have 12 posts with a total of 42 different words. The following words that have been tokenized will be counted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['better', 'billion', 'couldn', 'creat', 'cut', 'difficult', 'digit', 'discov', 'dog', 'echo', 'europ', 'figur', 'funni', 'good', 'happi', 'hide', 'horror', 'imagin', 'just', 'like', 'long', 'love', 'mighti', 'peopl', 'person', 'plan', 'planet', 'policeman', 'poverti', 'said', 'share', 'smile', 'tast', 'tell', 'thing', 'think', 'thought', 'use', 'walk', 'want', 'winter', 'world']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picks a random new post to find related posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinate matrix\n",
      "  (0, 41)\t0.5660249087784507\n",
      "  (0, 31)\t0.5660249087784507\n",
      "  (0, 23)\t0.5660249087784507\n",
      "  (0, 17)\t0.1970974579415979\n",
      "\n",
      "Full array\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.19709746\n",
      "  0.         0.         0.         0.         0.         0.56602491\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.56602491 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.56602491]]\n"
     ]
    }
   ],
   "source": [
    "new_post = \"Imagine all the people in the world smiling?\"\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "print(\"Coordinate matrix\")\n",
    "print(new_post_vec)\n",
    "print()\n",
    "print(\"Full array\")\n",
    "print(new_post_vec.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that count vectors returned by the **transform** method are sparse. That is, each vector does not store one count value for each word, as most of those counts would be zero (post does not contain the word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, using only the counts of the raw words is too simple. We will have to normalize them to get vectors of unit length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'calculates the Euclidean distance between the count vectors of the new post and all the old posts'\n",
    "import scipy as sp\n",
    "\n",
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1/sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2/sp.linalg.norm(v2.toarray())\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **norm( )** function calculates the Euclidean norm (shortest distance). With **dist_norm**, we just need to iterate over all the posts and remember the nearest one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post 0 with dist 1.37: I can only imagine how difficult this is for you.\n",
      "Post 1 with dist 1.27: Can you imagine that?\n",
      "Post 2 with dist 1.39: I can't imagine what he was thinking to hide a thing like that from you.\n",
      "Post 3 with dist 1.39: Imagine that you personally had to create everything you wanted to use.\n",
      "Post 4 with dist 1.39: He could imagine her horror when she discovered what he planned.\n",
      "Post 5 with dist 1.22: Then imagine if you shared your Digital Echo with a billion other people on the planet.\n",
      "Post 6 with dist 1.40: He cannot imagine how very, very happy he will be when he can tell us his thoughts, and we can tell him how we have loved him so long.\n",
      "Post 7 with dist 1.39: I imagine it would taste mighty good.\n",
      "Post 8 with dist 1.39: I can just imagine what a funny figure that policeman cut!\n",
      "Post 9 with dist 1.20: The winter's better here than Europe, I imagine, he said with a smile.\n",
      "Post 10 with dist 1.06: Can you imagine a world without poverty?\n",
      "Post 11 with dist 1.39: I couldn't imagine you'd take that long for a dog walk.\n",
      "\n",
      "Best post is 10 with dist 1.06\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "best_doc = None\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "\n",
    "for i in range(num_samples):\n",
    "    post = posts[i]\n",
    "    post_vec = X_train.getrow(i)\n",
    "    d = dist_norm(new_post_vec, post_vec)\n",
    "    print(\"Post %i with dist %.2f: %s\" % (i, d, post))\n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(\"\\nBest post is %i with dist %.2f\"%(best_i, best_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this process, we are able to convert a bunch of noisy text into a concise representation of feature values.\n",
    "But, as simple and as powerful as the bag-of-words approach with its extensions is, it has some drawbacks that we should be aware of. They are as follows:\n",
    "- It does not cover word relations. With the previous vectorization approach, the text \"Car hits wall\" and \"Wall hits car\" will both have the same feature vector.\n",
    "- It does not cover word relations. With the previous vectorization approach, the text \"Car hits wall\" and \"Wall hits car\" will both have the same feature vector.\n",
    "- It totally fails with misspelled words. Although it is clear to the readers that \"database\" and \"databas\" convey the same meaning, our approach will treat them as totally different words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. KMeans\n",
    "KMeans is the most widely used flat clustering algorithm. After it is initialized with the desired number of clusters, num_clusters, it maintains that number of so-called cluster centroids. Initially, it would pick any of the num_clusters posts and set the centroids to their feature vector. Then it would go through all other posts and assign them the nearest centroid as their current cluster. Then it will move each centroid into the middle of all the vectors of that particular class. This changes, of course, the cluster assignment. Some posts are now nearer to another cluster. So it will update the assignments for those changed posts. This is done as long as the centroids move a considerable amount. After some iterations, the movements will fall below a threshold and we consider clustering to be converged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset (20newsgroup)\n",
    "One standard dataset in machine learning is the 20newsgroup dataset, which contains 18,826 posts from 20 different newsgroups. Among the groups' topics are technical ones such as comp.sys.mac.hardware or sci.crypt as well as more politics- and religion-related ones such as talk.politics.guns or soc.religion. christian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "groups = ['comp.graphics', 'comp.os.ms-windows.misc', \n",
    "          'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', \n",
    "          'comp.windows.x', 'sci.space']\n",
    "\n",
    "train_data = fetch_20newsgroups(subset='train', categories=groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples: 3529, features: 4712\n"
     ]
    }
   ],
   "source": [
    "vectorizer = StemmedTfidfVectorizer(min_df=10, \n",
    "                                    max_df=0.5, \n",
    "                                    stop_words='english',\n",
    "                                    decode_error='ignore')\n",
    "vectorized = vectorizer.fit_transform(train_data.data)\n",
    "num_samples, num_features = vectorized.shape\n",
    "print(\"samples: %d, features: %d\" % (num_samples, num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a pool of **3529** posts and extracted for each of them a feature vector of **47121** dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 5915.679\n",
      "Iteration  1, inertia 3218.997\n",
      "Iteration  2, inertia 3180.855\n",
      "Iteration  3, inertia 3162.343\n",
      "Iteration  4, inertia 3152.263\n",
      "Iteration  5, inertia 3145.155\n",
      "Iteration  6, inertia 3140.431\n",
      "Iteration  7, inertia 3136.516\n",
      "Iteration  8, inertia 3133.709\n",
      "Iteration  9, inertia 3131.616\n",
      "Iteration 10, inertia 3130.326\n",
      "Iteration 11, inertia 3129.639\n",
      "Iteration 12, inertia 3129.083\n",
      "Iteration 13, inertia 3128.305\n",
      "Iteration 14, inertia 3127.844\n",
      "Iteration 15, inertia 3127.494\n",
      "Iteration 16, inertia 3127.022\n",
      "Iteration 17, inertia 3126.544\n",
      "Iteration 18, inertia 3126.386\n",
      "Iteration 19, inertia 3126.245\n",
      "Iteration 20, inertia 3126.132\n",
      "Iteration 21, inertia 3126.067\n",
      "Iteration 22, inertia 3125.982\n",
      "Iteration 23, inertia 3125.868\n",
      "Iteration 24, inertia 3125.730\n",
      "Iteration 25, inertia 3125.619\n",
      "Iteration 26, inertia 3125.596\n",
      "Converged at iteration 26: center shift 0.000000e+00 within tolerance 2.069005e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='random', max_iter=300,\n",
       "    n_clusters=50, n_init=1, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "'fix the cluster size to 50'\n",
    "num_clusters = 50\n",
    "km = KMeans(n_clusters=num_clusters, init='random', n_init=1,\n",
    "   verbose=1)\n",
    "km.fit(vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every vectorized post that has been fit, there is a corresponding integer label in **km.labels_**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34 37 27 ... 23 13 17]\n",
      "(3529,)\n"
     ]
    }
   ],
   "source": [
    "print(km.labels_)\n",
    "print(km.labels_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving our initial challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assign a cluster to a newly arriving post using **km.predict**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disk drive problems. Hi, I have a problem with my hard disk. After 1 year it is working only sporadically now. I tried to format it, but now it doesn't boot any more. Any ideas? Thanks.\n"
     ]
    }
   ],
   "source": [
    "new_post = \"Disk drive problems. Hi, I have a problem with my hard disk. After 1 year it is working only sporadically now. I tried to format it, but now it doesn't boot any more. Any ideas? Thanks.\"\n",
    "print(new_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectorize this post before we predict its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post_vec = vectorizer.transform([new_post])\n",
    "new_post_label = km.predict(new_post_vec)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the clustering, we do not need to compare **new_post_vec** to all post vectors. Instead, we can focus only on the posts of the same cluster. <br>\n",
    "The comparison in the bracket results in a Boolean array, and **nonzero** converts that\n",
    "array into a smaller array containing the indices of the **True** elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  66   69   71  125  157  201  213  214  225  233  247  308  351  354\n",
      "  359  370  384  392  395  463  479  531  533  565  580  581  618  676\n",
      "  689  714  731  779  806  807  905  935  939  944  961  964  976  987\n",
      " 1005 1114 1228 1242 1246 1266 1286 1313 1316 1388 1389 1427 1431 1481\n",
      " 1486 1487 1512 1519 1538 1548 1624 1637 1670 1716 1747 1752 1769 1840\n",
      " 1843 1852 1893 1986 1990 2010 2013 2061 2085 2139 2151 2223 2235 2257\n",
      " 2270 2277 2306 2347 2351 2400 2412 2414 2436 2447 2463 2475 2493 2512\n",
      " 2516 2525 2539 2565 2590 2612 2624 2651 2667 2678 2705 2745 2752 2791\n",
      " 2815 2842 2852 2951 2956 2964 2993 3018 3065 3145 3173 3186 3192 3202\n",
      " 3214 3219 3225 3285 3289 3296 3309 3321 3437 3450 3458]\n",
      "137\n"
     ]
    }
   ],
   "source": [
    "similar_indices = (km.labels_==new_post_label).nonzero()[0]\n",
    "print(similar_indices)\n",
    "print(len(similar_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **similar_indices**, we then simply have to build a list of posts together with\n",
    "their similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n"
     ]
    }
   ],
   "source": [
    "similar = []\n",
    "\n",
    "for i in similar_indices:\n",
    "    dist = sp.linalg.norm((new_post_vec - vectorized[i]).toarray())\n",
    "    similar.append((dist, train_data.data[i])) \n",
    "    \n",
    "similar = sorted(similar)\n",
    "print(len(similar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found **137** posts in the cluster of our post. To give the user a quick idea of what kind of similar posts are available, we can now present the most similar post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines shows the posts together with their similarity values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position 1\n",
      "Similarity 1.0378441731334074\n",
      "\n",
      "From: Thomas Dachsel <GERTHD@mvs.sas.com>\n",
      "Subject: BOOT PROBLEM with IDE controller\n",
      "Nntp-Posting-Host: sdcmvs.mvs.sas.com\n",
      "Organization: SAS Institute Inc.\n",
      "Lines: 25\n",
      "\n",
      "Hi,\n",
      "I've got a Multi I/O card (IDE controller + serial/parallel\n",
      "interface) and two floppy drives (5 1/4, 3 1/2) and a\n",
      "Quantum ProDrive 80AT connected to it.\n",
      "I was able to format the hard disk, but I could not boot from\n",
      "it. I can boot from drive A: (which disk drive does not matter)\n",
      "but if I remove the disk from drive A and press the reset switch,\n",
      "the LED of drive A: continues to glow, and the hard disk is\n",
      "not accessed at all.\n",
      "I guess this must be a problem of either the Multi I/o card\n",
      "or floppy disk drive settings (jumper configuration?)\n",
      "Does someone have any hint what could be the reason for it.\n",
      "Please reply by email to GERTHD@MVS.SAS.COM\n",
      "Thanks,\n",
      "Thomas\n",
      "+-------------------------------------------------------------------+\n",
      "| Thomas Dachsel                                                    |\n",
      "| Internet: GERTHD@MVS.SAS.COM                                      |\n",
      "| Fidonet:  Thomas_Dachsel@camel.fido.de (2:247/40)                 |\n",
      "| Subnet:   dachsel@rnivh.rni.sub.org (UUCP in Germany, now active) |\n",
      "| Phone:    +49 6221 4150 (work), +49 6203 12274 (home)             |\n",
      "| Fax:      +49 6221 415101                                         |\n",
      "| Snail:    SAS Institute GmbH, P.O.Box 105307, D-W-6900 Heidelberg |\n",
      "| Tagline:  One bad sector can ruin a whole day...                  |\n",
      "+-------------------------------------------------------------------+\n",
      "\n",
      "############################################################################################################\n",
      "Position 2\n",
      "Similarity 1.3072385649531673\n",
      "\n",
      "From: salmon@cwis.unomaha.edu (David Salmon)\n",
      "Subject: Re: HELP - SCSI Woes on Mac IIfx\n",
      "Organization: University of Nebraska at Omaha\n",
      "Lines: 16\n",
      "\n",
      "According to the official documentation, failure to use the IIfx terminator\n",
      "can not only affect SCSI bus performance but can also damage the bus.\n",
      "Whether this is your problem or not I don't know. I have had sporadic SCSI\n",
      "problems with my IIfx since I bought it. (I cannot connect more than three\n",
      "devices, fourth one causes major problems).\n",
      "\n",
      "First thing to do is to try to reformat your drive on someone elses system.\n",
      "If you continue to get errors it is probably the drive. If it formats fine\n",
      "then I would try to format it on your system with no externals. If this \n",
      "fails then the SCSI controller on your IIfx needs repair/replacement.\n",
      "\n",
      "Hope this helps.\n",
      " \n",
      "-- \n",
      "David C. Salmon\n",
      "salmon@unomaha.edu\n",
      "\n",
      "############################################################################################################\n",
      "Position 3\n",
      "Similarity 1.4142135623730951\n",
      "\n",
      "From: pburry@manitou.cse.dnd.ca (Paul Burry)\n",
      "Subject: Re: IDE vs SCSI\n",
      "Organization: Canadian System Security Centre\n",
      "Lines: 15\n",
      "\n",
      "In article <C5L6E7.2Dz4@austin.ibm.com> guyd@austin.ibm.com (Guy Dawson) writes:\n",
      "|> int eh same article the PC would will get plug and play SCSI {from the\n",
      "|> article it seems you get plug and play SCSI-1 only since SCSI-2 in FULL\n",
      "|> implimentation has TEN NOT 7 devices.}\n",
      "|\n",
      "|I beleive this last bit is just plain wrong!\n",
      "\n",
      "I believe you are right.  Both SCSI and SCSI-2 support 8 devices on the bus\n",
      "(normally that would be the host controller and 7 targets) each of which\n",
      "may have up to 8 logical units (LUNs).\n",
      "-- \n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "Paul Burry\t\t\t\n",
      "Voice: (613)-991-7325\t\tInternet: pburry@cse.dnd.ca\n",
      "Fax:   (613)-991-7323\t\tUUCP:\t  ..!{uunet,cunews}!cse.dnd.ca!pburry\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_at_1 = similar[0]\n",
    "show_at_2 = similar[len(similar)//2]\n",
    "show_at_3 = similar[-1]\n",
    "\n",
    "print('Position', 1)\n",
    "print('Similarity', show_at_1[0])\n",
    "print()\n",
    "print(show_at_1[1])\n",
    "print('#'*108)\n",
    "print('Position', 2)\n",
    "print('Similarity', show_at_2[0])\n",
    "print()\n",
    "print(show_at_2[1])\n",
    "print('#'*108)\n",
    "print('Position', 3)\n",
    "print('Similarity', show_at_3[0])\n",
    "print()\n",
    "print(show_at_3[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 LDL - Latent Dirichlet Allocation (Topic Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "\n",
    "corpus = corpora.BleiCorpus('ap/ap.dat', 'ap/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics= 15\n",
    "model = models.ldamodel.LdaModel(corpus,\n",
    "                                 num_topics=num_topics,\n",
    "                                 id2word=corpus.id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 0.029852249), (26, 0.034497377), (29, 0.01256772), (35, 0.027599448), (36, 0.07910758), (38, 0.21753761), (45, 0.039460044), (51, 0.024642373), (61, 0.01032077), (64, 0.3754707), (80, 0.10773616)]\n"
     ]
    }
   ],
   "source": [
    "topics = [model[c] for c in corpus]\n",
    "print(topics[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(59, 0.50087535)]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_document_topics(topics[400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.0040408657),\n",
       " (1, 0.0074477717),\n",
       " (2, 0.0063218917),\n",
       " (4, 0.0048942105),\n",
       " (7, 0.0049159424),\n",
       " (9, 0.004210106),\n",
       " (13, 0.003475071),\n",
       " (18, 0.0043182923),\n",
       " (143, 0.0035379431),\n",
       " (1622, 0.0037720285)]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(model.get_topic_terms(num_topics-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-259-486cfff3ebcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_document_topics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopics\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_document_topics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-259-486cfff3ebcf>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_document_topics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopics\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_document_topics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "[model.get_document_topics[topic] for topic in topics if len(model.ge == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 0.018344613),\n",
       " (20, 0.018087087),\n",
       " (27, 0.013613409),\n",
       " (36, 0.01096429),\n",
       " (86, 0.01764328)]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_term_topics(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {\"car\": [\"a\", \"b\", \"a\", \"c\", \"b\"], \"class\": [1,2,1,3,2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  car  class\n",
       "0  a   1    \n",
       "1  b   2    \n",
       "2  a   1    \n",
       "3  c   3    \n",
       "4  b   2    "
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'car': ['a', 'b', 'a', 'c', 'b'], 'class': [1, 2, 1, 3, 2]}"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regiment</th>\n",
       "      <th>company</th>\n",
       "      <th>name</th>\n",
       "      <th>preTestScore</th>\n",
       "      <th>postTestScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nighthawks</td>\n",
       "      <td>1st</td>\n",
       "      <td>Miller</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nighthawks</td>\n",
       "      <td>1st</td>\n",
       "      <td>Jacobson</td>\n",
       "      <td>24</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nighthawks</td>\n",
       "      <td>2nd</td>\n",
       "      <td>Ali</td>\n",
       "      <td>31</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nighthawks</td>\n",
       "      <td>2nd</td>\n",
       "      <td>Milner</td>\n",
       "      <td>2</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dragoons</td>\n",
       "      <td>1st</td>\n",
       "      <td>Cooze</td>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dragoons</td>\n",
       "      <td>1st</td>\n",
       "      <td>Jacon</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dragoons</td>\n",
       "      <td>2nd</td>\n",
       "      <td>Ryaner</td>\n",
       "      <td>24</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dragoons</td>\n",
       "      <td>2nd</td>\n",
       "      <td>Sone</td>\n",
       "      <td>31</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Scouts</td>\n",
       "      <td>1st</td>\n",
       "      <td>Sloan</td>\n",
       "      <td>2</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Scouts</td>\n",
       "      <td>1st</td>\n",
       "      <td>Piger</td>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Scouts</td>\n",
       "      <td>2nd</td>\n",
       "      <td>Riani</td>\n",
       "      <td>2</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Scouts</td>\n",
       "      <td>2nd</td>\n",
       "      <td>Ali</td>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      regiment company      name  preTestScore  postTestScore\n",
       "0   Nighthawks  1st     Miller    4             25           \n",
       "1   Nighthawks  1st     Jacobson  24            94           \n",
       "2   Nighthawks  2nd     Ali       31            57           \n",
       "3   Nighthawks  2nd     Milner    2             62           \n",
       "4   Dragoons    1st     Cooze     3             70           \n",
       "5   Dragoons    1st     Jacon     4             25           \n",
       "6   Dragoons    2nd     Ryaner    24            94           \n",
       "7   Dragoons    2nd     Sone      31            57           \n",
       "8   Scouts      1st     Sloan     2             62           \n",
       "9   Scouts      1st     Piger     3             70           \n",
       "10  Scouts      2nd     Riani     2             62           \n",
       "11  Scouts      2nd     Ali       3             70           "
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = {'regiment': ['Nighthawks', 'Nighthawks', 'Nighthawks', 'Nighthawks', 'Dragoons', 'Dragoons', 'Dragoons', 'Dragoons', 'Scouts', 'Scouts', 'Scouts', 'Scouts'], \n",
    "        'company': ['1st', '1st', '2nd', '2nd', '1st', '1st', '2nd', '2nd','1st', '1st', '2nd', '2nd'], \n",
    "        'name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze', 'Jacon', 'Ryaner', 'Sone', 'Sloan', 'Piger', 'Riani', 'Ali'], \n",
    "        'preTestScore': [4, 24, 31, 2, 3, 4, 24, 31, 2, 3, 2, 3],\n",
    "        'postTestScore': [25, 94, 57, 62, 70, 25, 94, 57, 62, 70, 62, 70]}\n",
    "df = pd.DataFrame(raw_data, columns = ['regiment', 'company', 'name', 'preTestScore', 'postTestScore'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dragoons': {'company': ['1st', '1st', '2nd', '2nd'], 'name': ['Cooze', 'Jacon', 'Ryaner', 'Sone']}, 'Nighthawks': {'company': ['1st', '1st', '2nd', '2nd'], 'name': ['Miller', 'Jacobson', 'Ali', 'Milner']}, 'Scouts': {'company': ['1st', '1st', '2nd', '2nd'], 'name': ['Sloan', 'Piger', 'Riani', 'Ali']}}\n"
     ]
    }
   ],
   "source": [
    "x = df.groupby('regiment')[['company','name']].apply(lambda x: x.to_dict(orient='list')).to_dict()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dragoons</th>\n",
       "      <th>Nighthawks</th>\n",
       "      <th>Scouts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>company</th>\n",
       "      <td>[1st, 1st, 2nd, 2nd]</td>\n",
       "      <td>[1st, 1st, 2nd, 2nd]</td>\n",
       "      <td>[1st, 1st, 2nd, 2nd]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>[Cooze, Jacon, Ryaner, Sone]</td>\n",
       "      <td>[Miller, Jacobson, Ali, Milner]</td>\n",
       "      <td>[Sloan, Piger, Riani, Ali]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Dragoons                       Nighthawks  \\\n",
       "company  [1st, 1st, 2nd, 2nd]          [1st, 1st, 2nd, 2nd]              \n",
       "name     [Cooze, Jacon, Ryaner, Sone]  [Miller, Jacobson, Ali, Milner]   \n",
       "\n",
       "                             Scouts  \n",
       "company  [1st, 1st, 2nd, 2nd]        \n",
       "name     [Sloan, Piger, Riani, Ali]  "
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.DataFrame(x)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dragoons</th>\n",
       "      <td>[1st, 1st, 2nd, 2nd]</td>\n",
       "      <td>[Cooze, Jacon, Ryaner, Sone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nighthawks</th>\n",
       "      <td>[1st, 1st, 2nd, 2nd]</td>\n",
       "      <td>[Miller, Jacobson, Ali, Milner]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scouts</th>\n",
       "      <td>[1st, 1st, 2nd, 2nd]</td>\n",
       "      <td>[Sloan, Piger, Riani, Ali]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         company                             name\n",
       "Dragoons    [1st, 1st, 2nd, 2nd]  [Cooze, Jacon, Ryaner, Sone]   \n",
       "Nighthawks  [1st, 1st, 2nd, 2nd]  [Miller, Jacobson, Ali, Milner]\n",
       "Scouts      [1st, 1st, 2nd, 2nd]  [Sloan, Piger, Riani, Ali]     "
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
